# Advanced Video Prediction Configuration
# Usage: python predict_advanced.py --config predict_advanced.yaml

# Inference Settings
inference:
  checkpoint: "/workspace/VDO_diffusion/runs/advanced_experiment_trial/best_model.pth"
  input_video: "/workspace/data/testvdo" # Can be a video file OR a directory of images
  output_dir: "./outputs/advanced_predictions"
  output_name: "prediction_result"
  device: "cuda"  # or "cpu"

# Video Settings (Must match training)
# Video Settings (Must match training)
video:
  num_frames: 6             # Total frames to generate (5 context + 1 future frame)
  num_context_frames: 5     # Number of frames to condition on
  frame_size: [256, 256]      # Height Width
  frame_interval: 1           # Sample every Nth frame

# VAE Settings (Must match training)
vae:
  latent_channels: 4
  vae_base_channels: 128
  vae_channel_mults: [1, 2, 4, 4]
  vae_temporal_downsample: [0, 0, 0, 0]
  spatial_downsample: 8
  temporal_downsample: 1

# DiT Architecture (Must match training)
dit:
  patch_size: [2, 2]
  hidden_dim: 768
  depth: 12
  num_heads: 12
  dim_head: 64
  ff_mult: 4
  dropout: 0.0
  num_classes: null

# Diffusion Process
diffusion:
  num_timesteps: 1000
  num_inference_steps: 100    # DDIM sampling steps (25/50/100/200 - higher = better quality but slower)
  beta_schedule: "cosine"
  prediction_type: "v"
  guidance_scale: 1.0         # Reduced from 7.5 to avoid noise amplification. Use 1.0 for unconditional sampling
  p_uncond: 0.1
  vae_loss_weight: 2.0
  kl_loss_weight: 0.0001

